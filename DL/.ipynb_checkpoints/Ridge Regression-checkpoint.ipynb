{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "\n",
    "    def __init__(self, epochs=200, alpha=0.1, batch_size=32):\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def compute_cost(self, X, y, w, alpha):\n",
    "        m = X.shape[0]\n",
    "        cost = (1. / (2. * m)) * (np.sum((np.dot(X, w) - y) ** 2.) + alpha * np.dot(w.T, w))\n",
    "            \n",
    "        return cost\n",
    "\n",
    "    def compute_gradient(self, X, y, w, epochs, alpha):\n",
    "\n",
    "        m = X.shape[0]\n",
    "        train_loss_lst = np.zeros((epochs, 1))\n",
    "\n",
    "        for i in range(epochs):\n",
    "            cost_lst = []\n",
    "            \n",
    "            # step through training sample by batch_size.\n",
    "            for j in range(0, X.shape[0], self.batch_size):\n",
    "                X_mini = X[j: j + self.batch_size]\n",
    "                y_mini = y[j: j + self.batch_size]\n",
    "                \n",
    "                cost= self.compute_cost(X_mini, y_mini, w, alpha)\n",
    "                cost_lst.append(cost)\n",
    "                w = w - (1 / m) * (np.dot(X_mini.T, (X_mini.dot(w) - y_mini[:, np.newaxis])) + alpha * w)\n",
    "            \n",
    "            train_loss = np.mean(np.array(cost))\n",
    "            train_loss_lst[i] = train_loss\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "\n",
    "                print(\"Epoch: \", i, \" , Loss: \", train_loss, \"\\n\")\n",
    "\n",
    "        return w, train_loss_lst\n",
    "\n",
    "    def train(self, X, y):\n",
    "\n",
    "        w = np.zeros((X.shape[1] + 1, 1))\n",
    "\n",
    "        # normalise the X, for each feature.\n",
    "        self.X_mean = np.mean(X, axis=0)\n",
    "        self.X_std = np.std(X, axis=0)\n",
    "        Xn = X - self.X_mean\n",
    "        self.X_std[self.X_std == 0] = 1\n",
    "        Xn /= self.X_std\n",
    "        \n",
    "        \n",
    "        self.y_mean = y.mean(axis=0)\n",
    "        yn = y - self.y_mean\n",
    "\n",
    "        # add ones for intercept term\n",
    "        Xn = np.hstack((np.ones(Xn.shape[0])[np.newaxis].T, Xn))\n",
    "\n",
    "        self.w, self.cost_lst = self.compute_gradient(Xn, yn, w, self.epochs, self.alpha)\n",
    "        \n",
    "        plt.plot(range(self.epochs), self.cost_lst, 'r--')\n",
    "        plt.legend(['Training Loss'])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xn = np.ndarray.copy(X)\n",
    "\n",
    "        Xn -= self.X_mean\n",
    "        Xn /= self.X_std\n",
    "        Xn = np.hstack((np.ones(Xn.shape[0])[np.newaxis].T, Xn))\n",
    "\n",
    "        return Xn.dot(self.w) + self.y_mean\n",
    "    \n",
    "rr = RidgeRegression()\n",
    "rr.train(trainFeat, trainYears)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
