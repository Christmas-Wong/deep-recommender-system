# Deep Neural Networks for YouTube Recommendations

评分：5+/5。
简介：使用DNN对大规模线上推荐系统架构的一次综述，包含Candidate Generation和Ranking两部分。Candidiate Gneration的部分负责生成user embedding，借鉴wordvec的skip gram negative sampling模型，Ranking部分使用类似的架构，并用weighted LR将目标改为预计观看时间。很经典的文章。

- DNN的方法可以看作是泛化的matrix factorization的类别，但是优势在于可以加入任意连续或者类别的特征，适合不断迭代。
- 在线系统获得的显式特征较稀疏，比如点赞、收藏等用户行为。训练时更多利用到的是隐式特征，比如用户观看市场，是否完整观看等等。
- CF中并没有强调次序的观念，而youtube的DNN做法则只选取在用户观看视频之前的N个操作数据作为输入。同时使用unordered bag来防止次序带来的直接影响。比如推荐用户刚搜索过的视频等。
- 鉴于用户点击视频事件highly unbalanced的属性，引入以用户观看时长为权重的weighted logistic regression，并推导可用odds来近似用户观看视频时长的预期（值域相同）。
- 线上serving的场景中，可以根据用户的最新操作实时更新user embedding，而video embedding则是SGNS训练中的副产物，需要定期全量重训。实时场景里将embedding存入内存用ANN的方法，比如LSH，来推荐视频，而不需要对全量视频跑inference。
- example age特征的引入很关键，指该用户操作距离此训练的时间间隔，使得模型可以模拟实际视频走红的时序特征。

# highlights

- first, we detail a deep candidate generation model and then describe a separate deep ranking model.

- Section 4 details the ranking model, including how classic logistic regression is modified to train a model predicting expected watch time (rather than click probability).

- Presenting a few “best” recommendations in a list requires a fine-level representation to distinguish relative importance among candidates with high recall.

- Furthermore, this design enables blending candidates generated by other sources, such as those described in an earlier work [3].

- J. Davidson, B. Liebald, J. Liu, P. Nandy, T. Van Vleet, U. Gargi, S. Gupta, Y. He, M. Lambert, B. Livingston, and D. Sampath. The youtube video recommendation system. In Proceedings of the Fourth ACM Conference on Recommender Systems, RecSys ’10, pages 293–296, New York, NY, USA, 2010. ACM.

- we can measure subtle changes in click-through rate, watch time, and many other metrics that measure user engagement. This is important because live A/B results are not always correlated with offline experiments.

- Early iterations of our neural network model mimicked this factorization behavior with shallow networks that only embedded the user’s previous watches.

- From this perspective, our approach can be viewed as a nonlinear generalization of factorization techniques.

- We pose recommendation as extreme multiclass classification where the prediction problem becomes accurately classifying a specific video watch wt at time t among millions of videos i (classes) from a corpus V based on a user U and context C,

- where u ∈ RNrepresents a high-dimensional “embedding” of the user, context pair and the vj ∈ RNrepresent embeddings of each candidate video. In this setting, an embedding is simply a mapping of sparse entities (individual videos, users etc.) into a dense vector in RN. The task of the deep neural network is to learn user embeddings u as a function of the user’s history and context that are useful for discriminating among videos with a softmax classifier.

- we use the implicit feedback [16] of watches to train the model, where a user completing a video is a positive example.

- , we rely on a technique to sample negative classes from the background distribution (“candidate sampling”) and then correct for this sampling via importance weighting [10].

- Scoring millions of items under a strict serving latency of tens of milliseconds requires an approximate scoring scheme sublinear in the number of classes. Previous systems at YouTube relied on hashing [24] and the classifier described here uses a similar approach. Since calibrated likelihoods from the softmax output layer are not needed at serving time, the scoring problem reduces to a nearest neighbor search in the dot product space for which general purpose libraries can be used [12]

- Inspired by continuous bag of words language models [14], we learn high dimensional embeddings for each video in a fixed vocabulary and feed these embeddings into a feedforward neural network.

- A key advantage of using deep neural networks as a generalization of matrix factorization is that arbitrary continuous and categorical features can be easily added to the model.

- Search history is treated similarly to watch history each query is tokenized into unigrams and bigrams and each token is embedded.

- Once averaged, the user’s tokenized, embedded queries represent a summarized dense search history.

- Demographic features are important for providing priors so that the recommendations behave reasonably for new users.

- The user’s geographic region and device are embedded and concatenated. Simple binary and continuous features such as the user’s gender, logged-in state and age are input directly into the network as real values normalized to [0, 1].

- “Example Age” Feature Many hours worth of videos are uploaded each second to YouTube. Recommending this recently uploaded (“fresh”) content is extremely important for YouTube as a product.

- We consistently observe that users prefer fresh content, though not at the expense of relevance.

- If users are discovering videos through means other than our recommendations, we want to be able to quickly propagate this discovery to others via collaborative filtering.

- Since our problem is posed as predicting the next watched video, a classifier given this information will predict that the most likely videos to be watched are those which appear on the corresponding search results page for “taylor swift”. Unsurpisingly, reproducing the user’s last search page as homepage recommendations performs very poorly.

- By discarding sequence information and representing search queries with an unordered bag of tokens, the classifier is no longer directly aware of the origin of the label.
